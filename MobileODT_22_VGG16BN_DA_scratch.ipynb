{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intel MobileODT Kaggle competition\n",
    "\n",
    "https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 1080 Ti (0000:01:00.0)\n",
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theano version: 0.9.0\n",
      "Keras version: 2.0.3\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from __future__ import print_function, division\n",
    "from importlib import reload \n",
    "import utils_p3; reload(utils_p3)\n",
    "from utils_p3 import *\n",
    "%matplotlib inline\n",
    "from IPython.display import FileLink\n",
    "import tensorflow as tf\n",
    "import six\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "from PIL import ImageFile\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "#print(\"TensorFlow version: %s\" % tf.__version__)\n",
    "print(\"Theano version: %s\" % theano.__version__)\n",
    "print(\"Keras version: %s\" % keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'channels_first'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HOME_DIR = os.getcwd()\n",
    "#path = \"data/imgs/\"\n",
    "path = \"data/testing/\"\n",
    "#path = \"data/testing/sample/\"\n",
    "batch_size = 1 # for pre-computation of last conv layer's output and the 5 augmented copies of training (OOM issue)\n",
    "#batch_size = 32 # for FCL and submission computation\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "keras.backend.image_data_format() #verify image_data_format for theano vs TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5048 images belonging to 3 classes.\n",
      "Found 1683 images belonging to 3 classes.\n",
      "Found 4018 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(path+'train', batch_size=batch_size, shuffle=False)\n",
    "val_batches = get_batches(path+'valid', batch_size=batch_size, shuffle=False)\n",
    "test_batches = get_batches(path+'test', batch_size=batch_size, shuffle=False)\n",
    "steps_per_epoch = int(np.ceil(batches.samples/batch_size))\n",
    "validation_steps = int(np.ceil(val_batches.samples/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5048 images belonging to 3 classes.\n",
      "Found 1683 images belonging to 3 classes.\n",
      "Found 4018 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Imagenet conv features with VGG16\n",
    "Based on code from Statefarm_original notebook of @Jeremy, lesson 4.\n",
    "\n",
    "Since we have so little data, and it is similar to imagenet images (full color photos), using pre-trained VGG weights is likely to be helpful - in fact it seems likely that we won't need to fine-tune the convolutional layer weights much, if at all.\n",
    "\n",
    "So we can pre-compute the output of the last convolutional layer, as we did in lesson 3 when we experimented with dropout. (However this means that we can't use full data augmentation, since we can't pre-compute something that changes every image.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import our class, using VGG16 with BatchNorm\n",
    "import vgg16bn_p3; reload(vgg16bn_p3) # *_p3 version code for Python 3.6 and Keras 2.0\n",
    "from vgg16bn_p3 import Vgg16BN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: pre-compute the output of the last convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grab VGG16 and find the last convolutional layer, we try to use 512x512 size over 224x224 original VGG16.\n",
    "vgg = Vgg16BN()\n",
    "model=vgg.model\n",
    "last_conv_idx = [i for i,l in enumerate(model.layers) if type(l) is Convolution2D][-1]\n",
    "conv_layers = model.layers[:last_conv_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a new model that includes everything up to that last convolutional layer\n",
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the outputs of that model by calculating the activations of that last convolutional layer\n",
    "conv_feat = conv_model.predict_generator(batches, int(np.ceil(batches.samples/batch_size)), workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As this takes time, save it to load it in the future\n",
    "save_array(path+'results/bn_conv_feat.dat', conv_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_val_feat = conv_model.predict_generator(val_batches, int(np.ceil(val_batches.samples/batch_size)), workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/bn_conv_val_feat.dat', conv_val_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_test_feat = conv_model.predict_generator(test_batches, int(np.ceil(test_batches.samples/batch_size)), workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/bn_conv_test_feat.dat', conv_test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can look at the original model and find the last convo layer \"conv2d_13\" with output shape (none, 512, 14, 14)\n",
    "# and compare it with the shape of of our new model's output.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's the same than \"conv2d_13\" !\n",
    "conv_val_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If this notebook was fully run once, we can directly reload the activations\n",
    "conv_feat = load_array(path+'results/bn_conv_feat.dat')\n",
    "conv_val_feat = load_array(path+'results/bn_conv_val_feat.dat')\n",
    "#conv_test_feat = load_array(path+'results/bn_conv_test_feat.dat')\n",
    "conv_val_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build new model on top, with dense layers\n",
    "Since we've pre-computed the output of the last convolutional layer, we need to create a network that takes that as input, and predicts our 3 classes.\n",
    "\n",
    "Let's try using a simplified version of VGG's dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we make 'p' a parameter to try different Dropout amounts\n",
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(3, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# quick test, verify batch_size\n",
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, epochs=1, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# full run on 15 epochs\n",
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, epochs=15, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.save_weights(path+'models/bn_conv22.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 3: Pre-computed data augmentation by making 5 augmented copies of training set\n",
    "\n",
    "We'll use our usual data augmentation parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5048 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Warning: uses HUGE amount of RAM (up to 60gb) and takes 90 mins for a simple 'batches.samples*2'\n",
    "# with batch_size=64. Check forum at\n",
    "# http://forums.fast.ai/t/state-farm-full-how-not-to-run-out-of-memory-with-vgg-da-batches-samples-5/3469/2\n",
    "# Maybe reduce batch_size and workers to 1 ? YES !!!\n",
    "# Also kernel needs a 'Restart and clear output' to clear RAM (now 15gb RAM + 12gb SWAP used),\n",
    "# so run Step 1 + 2 first, then reset and run Step 3.\n",
    "gen_t = image.ImageDataGenerator(rotation_range=8, height_shift_range=0.025,\n",
    "                                shear_range=0.05, channel_shift_range=10, width_shift_range=0.1)\n",
    "da_batches = get_batches(path+'train', gen_t, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_conv_feat = conv_model.predict_generator(da_batches, int(np.ceil(da_batches.samples*5)), workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/bn_da5_conv_feat.dat', da_conv_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include the real training data as well in its non-augmented form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_feat = load_array(path+'results/bn_conv_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_conv_feat = load_array(path+'results/bn_da5_conv_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_conv_feat_update = np.concatenate([da_conv_feat, conv_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/bn_da5_conv_feat_update.dat', da_conv_feat_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_conv_feat_update = load_array(path+'results/bn_da5_conv_feat_update.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since we've now got a dataset 6x bigger than before, we'll need to copy our labels 6 times too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_trn_labels = np.concatenate([trn_labels]*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/bn_da5_trn_labels.dat', da_trn_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "da_trn_labels = load_array(path+'results/bn_da5_trn_labels.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Based on some experiments the previous model works well, maybe with bigger dense layers like 512 later ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we make 'p' a parameter to try different Dropout amounts\n",
    "def get_bn_da_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(3, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_da_model = Sequential(get_bn_da_layers(p))\n",
    "bn_da_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_val_feat = load_array(path+'results/bn_conv_val_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30288 samples, validate on 1683 samples\n",
      "Epoch 1/1\n",
      "30288/30288 [==============================] - 58s - loss: 1.4089 - acc: 0.5964 - val_loss: 1.3028 - val_acc: 0.5573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd46225fb38>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test, verify batch_size\n",
    "bn_da_model.fit(da_conv_feat_update, da_trn_labels, batch_size=32, epochs=1, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30288 samples, validate on 1683 samples\n",
      "Epoch 1/50\n",
      "30288/30288 [==============================] - 27s - loss: 0.8133 - acc: 0.7295 - val_loss: 1.1924 - val_acc: 0.6554\n",
      "Epoch 2/50\n",
      "30288/30288 [==============================] - 28s - loss: 0.6449 - acc: 0.7860 - val_loss: 1.2107 - val_acc: 0.6649\n",
      "Epoch 3/50\n",
      "30288/30288 [==============================] - 27s - loss: 0.5768 - acc: 0.8185 - val_loss: 1.1218 - val_acc: 0.6863\n",
      "Epoch 4/50\n",
      "30288/30288 [==============================] - 27s - loss: 0.4990 - acc: 0.8430 - val_loss: 1.3277 - val_acc: 0.6821\n",
      "Epoch 5/50\n",
      "30288/30288 [==============================] - 27s - loss: 0.4303 - acc: 0.8627 - val_loss: 1.3184 - val_acc: 0.6833\n",
      "Epoch 6/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.3896 - acc: 0.8765 - val_loss: 1.2210 - val_acc: 0.7106\n",
      "Epoch 7/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.3702 - acc: 0.8862 - val_loss: 1.2278 - val_acc: 0.7011\n",
      "Epoch 8/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.3437 - acc: 0.8963 - val_loss: 1.3900 - val_acc: 0.6934\n",
      "Epoch 9/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.3071 - acc: 0.9073 - val_loss: 1.5131 - val_acc: 0.6993\n",
      "Epoch 10/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.2927 - acc: 0.9105 - val_loss: 1.3399 - val_acc: 0.7148\n",
      "Epoch 11/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.2725 - acc: 0.9169 - val_loss: 1.2929 - val_acc: 0.7142\n",
      "Epoch 12/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.2469 - acc: 0.9238 - val_loss: 1.4145 - val_acc: 0.7207\n",
      "Epoch 13/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.2386 - acc: 0.9285 - val_loss: 1.3802 - val_acc: 0.7106\n",
      "Epoch 14/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.2231 - acc: 0.9333 - val_loss: 1.2661 - val_acc: 0.7237\n",
      "Epoch 15/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.2036 - acc: 0.9371 - val_loss: 1.2747 - val_acc: 0.7445\n",
      "Epoch 16/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.2059 - acc: 0.9373 - val_loss: 1.2942 - val_acc: 0.7207\n",
      "Epoch 17/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1859 - acc: 0.9438 - val_loss: 1.2059 - val_acc: 0.7184\n",
      "Epoch 18/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1946 - acc: 0.9425 - val_loss: 1.3626 - val_acc: 0.7207\n",
      "Epoch 19/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1878 - acc: 0.9440 - val_loss: 1.3060 - val_acc: 0.7279\n",
      "Epoch 20/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1755 - acc: 0.9477 - val_loss: 1.3977 - val_acc: 0.7279\n",
      "Epoch 21/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1624 - acc: 0.9491 - val_loss: 1.2310 - val_acc: 0.7338\n",
      "Epoch 22/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1479 - acc: 0.9538 - val_loss: 1.3553 - val_acc: 0.7273\n",
      "Epoch 23/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1478 - acc: 0.9561 - val_loss: 1.2771 - val_acc: 0.7398\n",
      "Epoch 24/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1487 - acc: 0.9545 - val_loss: 1.3163 - val_acc: 0.7314\n",
      "Epoch 25/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1419 - acc: 0.9589 - val_loss: 1.2831 - val_acc: 0.7392\n",
      "Epoch 26/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1426 - acc: 0.9560 - val_loss: 1.2099 - val_acc: 0.7374\n",
      "Epoch 27/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1309 - acc: 0.9585 - val_loss: 1.3638 - val_acc: 0.7362\n",
      "Epoch 28/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1355 - acc: 0.9591 - val_loss: 1.3077 - val_acc: 0.7344\n",
      "Epoch 29/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1235 - acc: 0.9625 - val_loss: 1.2386 - val_acc: 0.7392\n",
      "Epoch 30/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1105 - acc: 0.9652 - val_loss: 1.4325 - val_acc: 0.7285\n",
      "Epoch 31/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1149 - acc: 0.9636 - val_loss: 1.3816 - val_acc: 0.7320\n",
      "Epoch 32/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1206 - acc: 0.9619 - val_loss: 1.2343 - val_acc: 0.7374\n",
      "Epoch 33/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1102 - acc: 0.9646 - val_loss: 1.2510 - val_acc: 0.7368\n",
      "Epoch 34/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1092 - acc: 0.9632 - val_loss: 1.3811 - val_acc: 0.7296\n",
      "Epoch 35/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1043 - acc: 0.9655 - val_loss: 1.3258 - val_acc: 0.7463\n",
      "Epoch 36/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1089 - acc: 0.9677 - val_loss: 1.3354 - val_acc: 0.7427\n",
      "Epoch 37/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1015 - acc: 0.9683 - val_loss: 1.3688 - val_acc: 0.7338\n",
      "Epoch 38/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.1049 - acc: 0.9687 - val_loss: 1.3059 - val_acc: 0.7362\n",
      "Epoch 39/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.0881 - acc: 0.9711 - val_loss: 1.2590 - val_acc: 0.7457\n",
      "Epoch 40/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.0907 - acc: 0.9710 - val_loss: 1.2542 - val_acc: 0.7421\n",
      "Epoch 41/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.0932 - acc: 0.9707 - val_loss: 1.2086 - val_acc: 0.7374\n",
      "Epoch 42/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.0949 - acc: 0.9692 - val_loss: 1.2284 - val_acc: 0.7398\n",
      "Epoch 43/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.0866 - acc: 0.9723 - val_loss: 1.2021 - val_acc: 0.7481\n",
      "Epoch 44/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.0813 - acc: 0.9743 - val_loss: 1.1718 - val_acc: 0.7421\n",
      "Epoch 45/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.0833 - acc: 0.9728 - val_loss: 1.2546 - val_acc: 0.7451\n",
      "Epoch 46/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.0869 - acc: 0.9718 - val_loss: 1.2634 - val_acc: 0.7421\n",
      "Epoch 47/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.0780 - acc: 0.9736 - val_loss: 1.1486 - val_acc: 0.7380\n",
      "Epoch 48/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.0788 - acc: 0.9744 - val_loss: 1.2558 - val_acc: 0.7445\n",
      "Epoch 49/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.0756 - acc: 0.9753 - val_loss: 1.2578 - val_acc: 0.7469\n",
      "Epoch 50/50\n",
      "30288/30288 [==============================] - 25s - loss: 0.0711 - acc: 0.9762 - val_loss: 1.3229 - val_acc: 0.7380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd463d9f4a8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full run on 15 epochs\n",
    "bn_da_model.fit(da_conv_feat_update, da_trn_labels, batch_size=32, epochs=50, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's save those weights.\n",
    "bn_da_model.save_weights(path+'models/bn_da5_conv22_50e.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's load those weights.\n",
    "bn_da_model.load_weights(path+'models/bn_da5_conv22_50e.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some test not saved with different learning rates or optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_da_model.compile(RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_da_model.fit(da_conv_feat_update, da_trn_labels, batch_size=batch_size, epochs=25, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import *\n",
    "bn_da_model.compile(Adagrad(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "bn_da_model.fit(da_conv_feat_update, da_trn_labels, batch_size=batch_size, epochs=25, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_da_model.compile(Adadelta(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "bn_da_model.fit(da_conv_feat_update, da_trn_labels, batch_size=batch_size, epochs=25, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_da_model.compile(Adamax(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "bn_da_model.fit(da_conv_feat_update, da_trn_labels, batch_size=batch_size, epochs=25, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_da_model.compile(Nadam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "bn_da_model.fit(da_conv_feat_update, da_trn_labels, batch_size=batch_size, epochs=25, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submissions\n",
    "\n",
    "Don't forget to add clipping for Kaggle submissions as it's very important to get the best cross_entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/9, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = bn_da_model.predict(conv_val_feat, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88522841921048512"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(keras.metrics.categorical_crossentropy(val_labels, do_clip(val_preds, 0.93)).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_test_feat = load_array(path+'results/bn_conv_test_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = bn_da_model.predict(conv_test_feat, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = do_clip(preds,0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm_name = path+'results/subm_22_bn_da_vgg_50e.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = sorted(batches.class_indices, key=batches.class_indices.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>Type_1</th>\n",
       "      <th>Type_2</th>\n",
       "      <th>Type_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.893303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name    Type_1  Type_2    Type_3\n",
       "0      0.jpg  0.007778  0.1044  0.893303\n",
       "1      1.jpg  0.007778  0.9300  0.007778\n",
       "2     10.jpg  0.007778  0.9300  0.007778\n",
       "3    100.jpg  0.007778  0.9300  0.007778\n",
       "4  10000.jpg  0.007778  0.9300  0.007778"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'image_name', [a[8:] for a in test_filenames])\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(subm_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='data/testing/results/subm_22_bn_da_vgg_50e.csv' target='_blank'>data/testing/results/subm_22_bn_da_vgg_50e.csv</a><br>"
      ],
      "text/plain": [
       "/media/eric/SSD500/fastai/deeplearning1/nbs/MobileODT/data/testing/results/subm_22_bn_da_vgg_50e.csv"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(subm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
