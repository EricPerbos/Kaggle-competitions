{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intel MobileODT Kaggle competition\n",
    "\n",
    "https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 1080 Ti (0000:01:00.0)\n",
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.0.3\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from __future__ import print_function, division\n",
    "from importlib import reload \n",
    "import utils_p3; reload(utils_p3)\n",
    "from utils_p3 import *\n",
    "%matplotlib inline\n",
    "from IPython.display import FileLink\n",
    "import tensorflow as tf\n",
    "import six\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "from PIL import ImageFile\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "#print(\"TensorFlow version: %s\" % tf.__version__)\n",
    "print(\"Keras version: %s\" % keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'channels_first'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HOME_DIR = os.getcwd()\n",
    "#path = \"data/imgs/\"\n",
    "path = \"data/testing/\"\n",
    "#path = \"data/testing/sample/\"\n",
    "train_valid_fraction = 0.75\n",
    "image_shape = (224,224)\n",
    "patience = 3\n",
    "batch_size = 64\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "keras.backend.image_data_format() #verify image_data_format for theano vs TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOME_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5048 images belonging to 3 classes.\n",
      "Found 1683 images belonging to 3 classes.\n",
      "Found 4018 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(path+'train', batch_size=batch_size, shuffle=False)\n",
    "val_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=False)\n",
    "test_batches = get_batches(path+'test', batch_size=batch_size*2, shuffle=False)\n",
    "steps_per_epoch = int(np.ceil(batches.samples/batch_size))\n",
    "validation_steps = int(np.ceil(val_batches.samples/(batch_size*2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5048 images belonging to 3 classes.\n",
      "Found 1683 images belonging to 3 classes.\n",
      "Found 4018 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Imagenet conv features with VGG16\n",
    "from Statefarm_original notebook\n",
    "\n",
    "Since we have so little data, and it is similar to imagenet images (full color photos), using pre-trained VGG weights is likely to be helpful - in fact it seems likely that we won't need to fine-tune the convolutional layer weights much, if at all.\n",
    "\n",
    "So we can pre-compute the output of the last convolutional layer, as we did in lesson 3 when we experimented with dropout. (However this means that we can't use full data augmentation, since we can't pre-compute something that changes every image.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import our class\n",
    "import vgg16_p3; reload(vgg16_p3)\n",
    "from vgg16_p3 import Vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: pre-compute the output of the last convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grab VGG16 and find the last convolutional layer\n",
    "vgg = Vgg16()\n",
    "model=vgg.model\n",
    "last_conv_idx = [i for i,l in enumerate(model.layers) if type(l) is Convolution2D][-1]\n",
    "conv_layers = model.layers[:last_conv_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a new model that includes everything up to that last convolutional layer\n",
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the outputs of that model by calculating the activations of that last convolutional layer\n",
    "conv_feat = conv_model.predict_generator(batches, int(np.ceil(batches.samples/batch_size)), workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As this takes time, save it to load it in the future\n",
    "save_array(path+'results/conv_feat.dat', conv_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_val_feat = conv_model.predict_generator(val_batches, int(np.ceil(val_batches.samples/(batch_size*2))), workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/conv_val_feat.dat', conv_val_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_test_feat = conv_model.predict_generator(test_batches, int(np.ceil(test_batches.samples/(batch_size*2))), workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/conv_test_feat.dat', conv_test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Augmentation: at 1:15:55 in L4 video, we can see Jeremy had a cell for gen_t then computed a da_conv_feat with nb_sample*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 3, 226, 226)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 224, 224)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 64, 226, 226)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 224, 224)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 112, 112)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 64, 114, 114)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 128, 112, 112)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 128, 114, 114)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 128, 112, 112)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 128, 56, 56)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 128, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 256, 56, 56)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 256, 28, 28)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin (None, 256, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 512, 28, 28)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPaddi (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 512, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 512, 7, 7)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We can look at the original model and find the last convo layer \"conv2d_13\" with output shape (none, 512, 14, 14)\n",
    "# and compare it with the shape of of our new model's output.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1683, 512, 14, 14)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's the same than \"conv2d_13\" !\n",
    "conv_val_feat.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "conv_feat = load_array(path+'results/conv_feat.dat')\n",
    "conv_val_feat = load_array(path+'results/conv_val_feat.dat')\n",
    "conv_val_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build new model on top, with dense layers\n",
    "Since we've pre-computed the output of the last convolutional layer, we need to create a network that takes that as input, and predicts our 3 classes. Let's try using a simplified version of VGG's dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we make 'p' a parameter to try different Dropout amounts\n",
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p/2),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(3, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5048 samples, validate on 1683 samples\n",
      "Epoch 1/1\n",
      "5048/5048 [==============================] - 1s - loss: 1.3708 - acc: 0.4580 - val_loss: 1.4255 - val_acc: 0.4450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f51f7078f28>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, epochs=1, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5048 samples, validate on 1683 samples\n",
      "Epoch 1/10\n",
      "5048/5048 [==============================] - 1s - loss: 0.8897 - acc: 0.6189 - val_loss: 1.0200 - val_acc: 0.5698\n",
      "Epoch 2/10\n",
      "5048/5048 [==============================] - 1s - loss: 0.6595 - acc: 0.7248 - val_loss: 1.0409 - val_acc: 0.5971\n",
      "Epoch 3/10\n",
      "5048/5048 [==============================] - 1s - loss: 0.4713 - acc: 0.8132 - val_loss: 1.1079 - val_acc: 0.6049\n",
      "Epoch 4/10\n",
      "5048/5048 [==============================] - 1s - loss: 0.3389 - acc: 0.8651 - val_loss: 1.1980 - val_acc: 0.6001\n",
      "Epoch 5/10\n",
      "5048/5048 [==============================] - 1s - loss: 0.2709 - acc: 0.9015 - val_loss: 1.2498 - val_acc: 0.6405\n",
      "Epoch 6/10\n",
      "5048/5048 [==============================] - 1s - loss: 0.2213 - acc: 0.9166 - val_loss: 1.3499 - val_acc: 0.6150\n",
      "Epoch 7/10\n",
      "5048/5048 [==============================] - 1s - loss: 0.1725 - acc: 0.9394 - val_loss: 1.4695 - val_acc: 0.6078\n",
      "Epoch 8/10\n",
      "5048/5048 [==============================] - 1s - loss: 0.1601 - acc: 0.9433 - val_loss: 1.4441 - val_acc: 0.6168\n",
      "Epoch 9/10\n",
      "5048/5048 [==============================] - 1s - loss: 0.1529 - acc: 0.9426 - val_loss: 1.5202 - val_acc: 0.6292\n",
      "Epoch 10/10\n",
      "5048/5048 [==============================] - 1s - loss: 0.1205 - acc: 0.9600 - val_loss: 1.5282 - val_acc: 0.6286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f51ee11a0b8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, epochs=10, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5048 samples, validate on 1683 samples\n",
      "Epoch 1/25\n",
      "5048/5048 [==============================] - 1s - loss: 1.1829 - acc: 0.4913 - val_loss: 1.1901 - val_acc: 0.4670\n",
      "Epoch 2/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.8251 - acc: 0.6313 - val_loss: 1.1027 - val_acc: 0.5674\n",
      "Epoch 3/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.5870 - acc: 0.7526 - val_loss: 1.1279 - val_acc: 0.5847\n",
      "Epoch 4/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.4156 - acc: 0.8360 - val_loss: 1.4486 - val_acc: 0.5966\n",
      "Epoch 5/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.3014 - acc: 0.8811 - val_loss: 1.4433 - val_acc: 0.5977\n",
      "Epoch 6/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.2606 - acc: 0.9083 - val_loss: 1.4661 - val_acc: 0.6156\n",
      "Epoch 7/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.1904 - acc: 0.9291 - val_loss: 1.5888 - val_acc: 0.6084\n",
      "Epoch 8/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.1710 - acc: 0.9386 - val_loss: 1.8804 - val_acc: 0.6025\n",
      "Epoch 9/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.1515 - acc: 0.9447 - val_loss: 1.7624 - val_acc: 0.6084\n",
      "Epoch 10/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.1576 - acc: 0.9459 - val_loss: 1.9192 - val_acc: 0.6061\n",
      "Epoch 11/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.1227 - acc: 0.9588 - val_loss: 1.7847 - val_acc: 0.6179\n",
      "Epoch 12/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.1098 - acc: 0.9604 - val_loss: 1.8373 - val_acc: 0.6245\n",
      "Epoch 13/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.1080 - acc: 0.9616 - val_loss: 1.8751 - val_acc: 0.6168\n",
      "Epoch 14/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.0968 - acc: 0.9677 - val_loss: 2.0067 - val_acc: 0.5912\n",
      "Epoch 15/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.1186 - acc: 0.9600 - val_loss: 2.1532 - val_acc: 0.6102\n",
      "Epoch 16/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.1048 - acc: 0.9626 - val_loss: 1.9100 - val_acc: 0.6269\n",
      "Epoch 17/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.0889 - acc: 0.9707 - val_loss: 1.9603 - val_acc: 0.6150\n",
      "Epoch 18/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.1024 - acc: 0.9651 - val_loss: 1.9004 - val_acc: 0.6251\n",
      "Epoch 19/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.0813 - acc: 0.9729 - val_loss: 1.9382 - val_acc: 0.6209\n",
      "Epoch 20/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.0777 - acc: 0.9735 - val_loss: 1.9237 - val_acc: 0.6203\n",
      "Epoch 21/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.0612 - acc: 0.9796 - val_loss: 2.1146 - val_acc: 0.6275\n",
      "Epoch 22/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.0653 - acc: 0.9768 - val_loss: 2.2200 - val_acc: 0.6257\n",
      "Epoch 23/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.0747 - acc: 0.9739 - val_loss: 2.0586 - val_acc: 0.6221\n",
      "Epoch 24/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.0881 - acc: 0.9685 - val_loss: 1.8711 - val_acc: 0.6096\n",
      "Epoch 25/25\n",
      "5048/5048 [==============================] - 1s - loss: 0.0778 - acc: 0.9742 - val_loss: 2.0114 - val_acc: 0.6275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f51d44de898>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, epochs=25, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submissions\n",
    "\n",
    "Don't forget to add clipping for Kaggle submissions as it's very important to get the best cross_entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/9, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_preds = bn_model.predict(conv_val_feat, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4063674341234258"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(keras.metrics.categorical_crossentropy(val_labels, do_clip(val_preds, 0.93)).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_test_feat = load_array(path+'results/conv_test_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = bn_model.predict(conv_test_feat, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = do_clip(preds,0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm_name = path+'results/subm_14_vgg.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = sorted(batches.class_indices, key=batches.class_indices.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>Type_1</th>\n",
       "      <th>Type_2</th>\n",
       "      <th>Type_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.jpg</td>\n",
       "      <td>0.057717</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         img    Type_1    Type_2    Type_3\n",
       "0      0.jpg  0.007778  0.930000  0.007778\n",
       "1      1.jpg  0.007778  0.007778  0.930000\n",
       "2     10.jpg  0.007778  0.930000  0.007778\n",
       "3    100.jpg  0.057717  0.930000  0.007778\n",
       "4  10000.jpg  0.007778  0.930000  0.007778"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'img', [a[8:] for a in test_filenames])\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(subm_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='data/testing/results/subm_14_vgg.csv' target='_blank'>data/testing/results/subm_14_vgg.csv</a><br>"
      ],
      "text/plain": [
       "/media/eric/SSD500/fastai/deeplearning1/nbs/MobileODT/data/testing/results/subm_14_vgg.csv"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(subm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
